{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKkeYE20a37b"
      },
      "source": [
        "# Winter 2026 COMP 579 Assignment 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Group: \n",
        "\n",
        "Name:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfZPHeT9i946"
      },
      "source": [
        "1. Do Not Change the Random Seed\n",
        "The random seed has been set to ensure reproducibility. Please do not modify it.\n",
        "\n",
        "2. Guidance for the First Question\n",
        "For the initial question, fill in the blanks under the sections marked as TODO. Follow the provided structure and complete the missing parts.\n",
        "\n",
        "3. Approach for Subsequent Questions\n",
        "For the later questions, we expect you to attempt the solutions independently. You can refer to the examples provided in earlier questions to understand how to\n",
        "plot figures and implement solutions.\n",
        "\n",
        "4. Ensure that the plots you produce for later questions are similar in style and format to those shown in the previous examples.\n",
        "\n",
        "5. Answer questions in the Answer block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "eEvd8WcFqvai"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "np.random.seed(40)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=10,5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss77N5TbLVIl"
      },
      "source": [
        "## Q1 Simulator for Bernoulli Bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG8suY4Sn7hu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BernoulliBandit:\n",
        "  \"\"\"\n",
        "    A class representing a Bernoulli multi-armed bandit.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    num_arms : int\n",
        "        Number of arms in the bandit.\n",
        "    probs : list or np.ndarray\n",
        "        List of success probabilities for each arm (values between 0 and 1).\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    sample(arm_index)\n",
        "        Samples a reward (0 or 1) from the specified arm based on a Bernoulli distribution.\n",
        "    \"\"\"\n",
        "\n",
        "  # TODO:\n",
        "  def __init__(self, num_arms, probs=None):\n",
        "    self.num_arms = \n",
        "    self.probs = \n",
        "\n",
        "  def sample(self, arm_index):\n",
        "    return \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2G0G_s5sy_C"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "delta = \n",
        "num_arms = \n",
        "probs = \n",
        "num_samples =\n",
        "\n",
        "three_arm_bernoulli_bandit = \n",
        "\n",
        "# Store the rewards for each arm\n",
        "action_rewards = []\n",
        "actions = range(num_arms)\n",
        "\n",
        "for action in actions:\n",
        "    # Store 50 samples per action\n",
        "    rewards = \n",
        "    action_rewards.append(rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG_coTYL1_RL"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pj04OZ0ozUjK",
        "outputId": "180a1ae7-d992-444f-f60a-7a81a1f71bb3"
      },
      "outputs": [],
      "source": [
        "for action in actions:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # TODO:\n",
        "  true_value = \n",
        "  estimated_value = \n",
        "\n",
        "  # draw the line of the true value\n",
        "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
        "  # draw the line of the estimated value\n",
        "  line_est_val = ax.axhline(y = estimated_value, color = 'r', linestyle = '--', label = \"estimated value\")\n",
        "  # plot the reward samples\n",
        "  plt_samples, = ax.plot(action_rewards[action], 'o', label = \"reward samples\")\n",
        "\n",
        "  ax.set_xlabel(\"sample number\")\n",
        "  ax.set_ylabel(\"reward value\")\n",
        "  ax.set_title(\"Sample reward, estimated and true expected reward over 50 samples for action %s\" %action, y=-0.2)\n",
        "\n",
        "  # show the legend with the labels of the line\n",
        "  ax.legend(handles=[line_true_val, line_est_val, plt_samples])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKLg6U5bLhRs"
      },
      "source": [
        "## Q2 Estimated Q values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgnKe19V0NgR"
      },
      "outputs": [],
      "source": [
        "def update(reward_samples, alpha):\n",
        "  \"\"\"Yields incremental average with fixed learning rate alpha.\"\"\"\n",
        "  Q = \n",
        "  \n",
        "    yield Q\n",
        "\n",
        "def updateAvg(reward_samples):\n",
        "  \"\"\"Yields incremental sample average.\"\"\"\n",
        "  Q = \n",
        "  \n",
        "    yield Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmG74R11x-j"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G_ltKRTcBaDM",
        "outputId": "90ff668b-afde-4064-f2f3-62625494f41e"
      },
      "outputs": [],
      "source": [
        "for action in actions:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # TODO:\n",
        "  incr_avgs = \n",
        "  alpha_1_percent = list(update(action_rewards[action], alpha=0.01))\n",
        "  alpha_5_percent = \n",
        "  alpha_10_percent = \n",
        "  true_value = \n",
        "\n",
        "  # draw the true value line\n",
        "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
        "\n",
        "  # plot incremental values for averaging, alpha = 0.01, alpha = 0.1\n",
        "  plt_incr_avgs, = ax.plot(incr_avgs, label = \"incremental average\")\n",
        "  plt_alpha_1_percent, = ax.plot(alpha_1_percent, label = r\"$\\alpha = 0.01$\")\n",
        "  plt_alpha_5_percent, = ax.plot(alpha_5_percent, label = r\"$\\alpha = 0.05$\")\n",
        "  plt_alpha_10_percent, = ax.plot(alpha_10_percent, label = r\"$\\alpha = 0.1$\")\n",
        "\n",
        "  ax.set_xlabel(\"number of samples\")\n",
        "  ax.set_ylabel(\"estimated q value\")\n",
        "  ax.set_title(\"Estimated q value vs number of samples for action %s\" %action, y=-0.2)\n",
        "\n",
        "  # show the legend with the labels of the line\n",
        "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_5_percent, plt_alpha_10_percent])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMdCH4-lLw44"
      },
      "source": [
        "## Q3 Effect of $α$ on Estimated Q values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6_tT59BB1Ro"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "num_samples = \n",
        "num_runs = \n",
        "\n",
        "incr_avgs_runs = []\n",
        "alpha_1_percent_runs = []\n",
        "alpha_5_percent_runs = []\n",
        "alpha_10_percent_runs = []\n",
        "\n",
        "\n",
        "# TODO:\n",
        "for run in range(num_runs):\n",
        "  sample_incr_avgs_by_actions = []\n",
        "  sample_alpha_1_percent_by_actions = []\n",
        "  sample_alpha_5_percent_by_actions = []\n",
        "  sample_alpha_10_percent_by_actions = []\n",
        "\n",
        "  for action in actions:\n",
        "    rewards = \n",
        "    sample_incr_avgs_by_actions.append()\n",
        "    sample_alpha_1_percent_by_actions.append()\n",
        "    sample_alpha_5_percent_by_actions.append()\n",
        "    sample_alpha_10_percent_by_actions.append()\n",
        "\n",
        "  incr_avgs_runs.append(sample_incr_avgs_by_actions)\n",
        "  alpha_1_percent_runs.append(sample_alpha_1_percent_by_actions)\n",
        "  alpha_5_percent_runs.append(sample_alpha_5_percent_by_actions)\n",
        "  alpha_10_percent_runs.append(sample_alpha_10_percent_by_actions)\n",
        "\n",
        "# convert to np arrays\n",
        "incr_avgs_runs = np.asarray(incr_avgs_runs)\n",
        "alpha_1_percent_runs = np.asarray(alpha_1_percent_runs)\n",
        "alpha_5_percent_runs = np.asarray(alpha_5_percent_runs)\n",
        "alpha_10_percent_runs = np.asarray(alpha_10_percent_runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaDSMygu2IZc"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EUB4pwXaRM_E",
        "outputId": "101d28ba-01d9-4661-ca81-8d9ddce090a0"
      },
      "outputs": [],
      "source": [
        "from pdb import set_trace as bp\n",
        "for action in actions:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # obtain averaged incremental reward values\n",
        "  # TODO:\n",
        "  mean_incr_avgs_by_actions = \n",
        "  mean_alpha_1_percent_by_actions = \n",
        "  mean_alpha_5_percent_by_actions = \n",
        "  mean_alpha_10_percent_by_actions = \n",
        "\n",
        "  true_value = probs[action]\n",
        "\n",
        "  std_err_incr_avgs_by_actions = \n",
        "  std_err_alpha_1_percent_by_actions = \n",
        "  std_err_alpha_5_percent_by_actions = \n",
        "  std_err_alpha_10_percent_by_actions = \n",
        "\n",
        "\n",
        "  # draw the true value line\n",
        "  line_true_val = ax.axhline(y = true_value, color = 'b', linestyle = ':', label = \"true value\")\n",
        "\n",
        "  # draw the averaged incremental reward values for averaging\n",
        "  plt_incr_avgs, = ax.plot(mean_incr_avgs_by_actions, label = \"incremental average\")\n",
        "  # draw the error bar/area for averaging\n",
        "  incr_avgs_minus_std_err = mean_incr_avgs_by_actions - std_err_incr_avgs_by_actions\n",
        "  incr_avgs_plus_std_err = mean_incr_avgs_by_actions + std_err_incr_avgs_by_actions\n",
        "  ax.fill_between(range(0,100), incr_avgs_minus_std_err, incr_avgs_plus_std_err, alpha=0.3)\n",
        "\n",
        "  # draw the averaged incremental reward values for alpha = 0.01\n",
        "  plt_alpha_1_percent, = ax.plot(mean_alpha_1_percent_by_actions, label = r\"$\\alpha = 0.01$\")\n",
        "  # draw the error bar/area for alpha = 0.01\n",
        "  alpha_1_percent_minus_std_err = mean_alpha_1_percent_by_actions - std_err_alpha_1_percent_by_actions\n",
        "  alpha_1_percent_plus_std_err = mean_alpha_1_percent_by_actions + std_err_alpha_1_percent_by_actions\n",
        "  ax.fill_between(range(0,100), alpha_1_percent_minus_std_err, alpha_1_percent_plus_std_err, alpha=0.3)\n",
        "\n",
        "  # draw the averaged incremental reward values for alpha = 0.05\n",
        "  plt_alpha_5_percent, = ax.plot(mean_alpha_5_percent_by_actions, label = r\"$\\alpha = 0.05$\")\n",
        "  # draw the error bar/area for alpha = 0.05\n",
        "  alpha_5_percent_minus_std_err = mean_alpha_5_percent_by_actions - std_err_alpha_5_percent_by_actions\n",
        "  alpha_5_percent_plus_std_err = mean_alpha_5_percent_by_actions + std_err_alpha_5_percent_by_actions\n",
        "  ax.fill_between(range(0,100), alpha_5_percent_minus_std_err, alpha_5_percent_plus_std_err, alpha=0.3)\n",
        "\n",
        "  # draw the averaged incremental reward values for alpha = 0.1\n",
        "  plt_alpha_10_percent, = ax.plot(mean_alpha_10_percent_by_actions, label = r\"$\\alpha = 0.1$\")\n",
        "  # draw the error bar/area for alpha = 0.1\n",
        "  alpha_10_percent_minus_std_err = mean_alpha_10_percent_by_actions - std_err_alpha_10_percent_by_actions\n",
        "  alpha_10_percent_plus_std_err = mean_alpha_10_percent_by_actions + std_err_alpha_10_percent_by_actions\n",
        "  ax.fill_between(range(0,100), alpha_10_percent_minus_std_err, alpha_10_percent_plus_std_err, alpha=0.3)\n",
        "\n",
        "  ax.set_xlabel(\"number of samples\")\n",
        "  ax.set_ylabel(\"estimated q value\")\n",
        "  ax.set_title(\"Average estimated q value over 100 runs for action %s\" %action, y=-0.2)\n",
        "\n",
        "  ax.legend(handles=[line_true_val, plt_incr_avgs, plt_alpha_1_percent, plt_alpha_5_percent, plt_alpha_10_percent])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HKeI_j9cpvs"
      },
      "source": [
        "### Answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL_sU0R5L2se"
      },
      "source": [
        "## Q4 Epsilon-greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEhRJLpKdhK0"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(bandit, epsilon, alpha = None, num_time_step = 1000, epsilon_decay=False, lambda_=0.001):\n",
        "  \"\"\"Epsilon greedy algorithm for bandit action selection\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  bandit : bandit class\n",
        "      A bernoulli bandit attributes num_arms and probs_arr, and method sample\n",
        "  epsilon: float\n",
        "      A parameter which determines the probability for a random action to be selected\n",
        "  alpha: (optional) float\n",
        "      A parameter which determined the learning rate for averaging. If alpha is none, incremental averaging is used.\n",
        "      Default is none, corresponding to incremental averaging.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  R_over_t\n",
        "      a list of instantaneous return over the time steps\n",
        "  total_R_over_t\n",
        "      a list of cummulative reward over the time steps\n",
        "  est_is_best_over_t\n",
        "      a list of values of 0 and 1 where 1 indicates the estimated best action is the true best action and 0 otherwise for each time step\n",
        "  l_over_t\n",
        "      a list of instanteneous regret over the time steps\n",
        "  total_l_over_t\n",
        "      a list of cummulative regret over the time steps\n",
        "  \"\"\"\n",
        "  # TODO:\n",
        "  num_arms = \n",
        "\n",
        "  Q_arr = \n",
        "  N_arr = \n",
        "  total_R = \n",
        "  total_l = \n",
        "  actions = \n",
        "\n",
        "  R_over_t = []\n",
        "  total_R_over_t = []\n",
        "  est_is_best_over_t = []\n",
        "  l_over_t = []\n",
        "  total_l_over_t = []\n",
        "\n",
        "  epsilon_t = epsilon\n",
        "\n",
        "  for time_step in range(num_time_step):\n",
        "    if epsilon_decay:\n",
        "        epsilon_t = \n",
        "\n",
        "    opt_value = \n",
        "    best_action = \n",
        "\n",
        "    A_star = \n",
        "    A_random = \n",
        "    A = \n",
        "\n",
        "    curr_R = \n",
        "    N_arr[A] = \n",
        "\n",
        "    if alpha == None:\n",
        "      # incremental averaging\n",
        "      Q_arr[A] = \n",
        "    else:\n",
        "      Q_arr[A] = \n",
        "\n",
        "    R_over_t.append(curr_R)\n",
        "\n",
        "    total_R = \n",
        "    avg_R = \n",
        "    total_R_over_t.append(avg_R)\n",
        "\n",
        "    est_is_best = \n",
        "    est_is_best_over_t.append(est_is_best)\n",
        "\n",
        "    l_t = \n",
        "    l_over_t.append(l_t)\n",
        "\n",
        "    total_l = \n",
        "    total_l_over_t.append(total_l)\n",
        "\n",
        "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU1_pP7INeBH"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JgvBlfQDBJdv",
        "outputId": "99c72e26-a678-431f-a708-c45151faec9c"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "epsilons = \n",
        "decaying_epsilon_params = {'epsilon_0': , 'lambda_': }  # Decaying epsilon parameters\n",
        "\n",
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
        "\n",
        "for epsilon in epsilons + [\"decay\"]:\n",
        "\n",
        "  # arrays of the data generated from 100 runs\n",
        "  R_over_t_runs = []\n",
        "  total_R_over_t_runs = []\n",
        "  est_is_best_over_t_runs = []\n",
        "  l_over_t_runs = []\n",
        "  total_l_over_t_runs = []\n",
        "\n",
        "  for run in range(100):\n",
        "    if epsilon == \"decay\":\n",
        "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
        "          three_arm_bernoulli_bandit,\n",
        "          decaying_epsilon_params['epsilon_0'],\n",
        "          epsilon_decay=True,\n",
        "          lambda_=decaying_epsilon_params['lambda_']\n",
        "      )\n",
        "    else:\n",
        "      R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
        "          three_arm_bernoulli_bandit,\n",
        "          epsilon\n",
        "      )\n",
        "    R_over_t_runs.append(R_over_t)\n",
        "    total_R_over_t_runs.append(total_R_over_t)\n",
        "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
        "    l_over_t_runs.append(l_over_t)\n",
        "    total_l_over_t_runs.append(total_l_over_t)\n",
        "\n",
        "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
        "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
        "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
        "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
        "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
        "\n",
        "  # plot the mean reward over time\n",
        "\n",
        "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
        "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(np.size(R_over_t_runs, axis=0))\n",
        "\n",
        "  axs[0,0].plot(mean_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  R_over_t_minus_std_err = mean_R_over_t_runs - std_err_R_over_t_runs\n",
        "  R_over_t_plus_std_err = mean_R_over_t_runs  + std_err_R_over_t_runs\n",
        "  axs[0,0].fill_between(range(0,1000), R_over_t_minus_std_err, R_over_t_plus_std_err, alpha=0.4)\n",
        "  # axs[0,0].errorbar(range(0,1000), mean_R_over_t_runs, yerr=std_err_R_over_t_runs)\n",
        "\n",
        "  axs[0,0].legend()\n",
        "  axs[0,0].set_xlabel(\"time step\")\n",
        "  axs[0,0].set_ylabel(\"reward value\")\n",
        "  axs[0,0].set_title(\"Average Instanteneous Reward Received over Time\", y=-0.18)\n",
        "\n",
        "  # plot the mean cummulative reward over time\n",
        "\n",
        "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
        "  std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(np.size(total_R_over_t_runs, axis=0))\n",
        "\n",
        "  axs[0,1].plot(mean_total_R_over_t_runs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  total_R_over_t_minus_std_err = mean_total_R_over_t_runs - std_err_total_R_over_t_runs\n",
        "  total_R_over_t_plus_std_err = mean_total_R_over_t_runs  + std_err_total_R_over_t_runs\n",
        "  axs[0,1].fill_between(range(0,1000), total_R_over_t_minus_std_err, total_R_over_t_plus_std_err, alpha=0.4)\n",
        "\n",
        "  axs[0,1].legend()\n",
        "  axs[0,1].set_xlabel(\"time step\")\n",
        "  axs[0,1].set_ylabel(\"average reward\")\n",
        "  axs[0,1].set_title(\"Average Reward up to Time Step t\", y=-0.18)\n",
        "\n",
        "  #plot the mean percentage of the estimated best action being the first action\n",
        "\n",
        "  est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
        "  plt_est_is_best_over_t_runs_avgs, = axs[1,0].plot(est_is_best_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  axs[1,0].legend()\n",
        "  axs[1,0].set_xlabel(\"time step\")\n",
        "  axs[1,0].set_ylabel(\"fraction\")\n",
        "  axs[1,0].set_title(\"Fraction of Runs where Estimated Best Action is the Third Action\", y=-0.18)\n",
        "\n",
        "  #plot the mean instantaneous regret over time\n",
        "\n",
        "  l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
        "  axs[1,1].plot(l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  axs[1,1].legend()\n",
        "  axs[1,1].set_xlabel(\"time step\")\n",
        "  axs[1,1].set_ylabel(\"regret\")\n",
        "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
        "\n",
        "  #plot the total regret over time\n",
        "\n",
        "  total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
        "  axs[2,0].plot(total_l_over_t_runs_avgs, label = r\"$\\epsilon = %s$\" %epsilon)\n",
        "\n",
        "  axs[2,0].legend()\n",
        "  axs[2,0].set_xlabel(\"time step\")\n",
        "  axs[2,0].set_ylabel(\"regret\")\n",
        "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
        "\n",
        "axs[-1, -1].axis('off')\n",
        "\n",
        "title = r'Graphs  for Epsilon Greedy with Varying Epsilons'\n",
        "fig.suptitle(title, fontsize=16, y=0.08)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILoBgOrocu_z"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnrJI7uKAvkH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ttd7oJXiQe"
      },
      "source": [
        "## Q6 Gradient Bandit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqMJ_nE8s-dm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wed5NdLZXjrE"
      },
      "outputs": [],
      "source": [
        "def gradient_bandit(bandit, temperature, num_steps=1000, baseline=True):\n",
        "  \"\"\"Gradient bandit algorithm with softmax (Boltzmann) action selection.\"\"\"\n",
        "  #TODO:\n",
        "  num_arms = \n",
        "\n",
        "  Q = \n",
        "  N = \n",
        "  b = \n",
        "\n",
        "  R_over_t = []\n",
        "  total_R_over_t = []\n",
        "  est_is_best_over_t = []\n",
        "  l_over_t = []\n",
        "  total_l_over_t = []\n",
        "  total_R = \n",
        "  total_l = \n",
        "\n",
        "  for t in range(num_steps):\n",
        "    optimal_arm =\n",
        "    optimal_reward = \n",
        "\n",
        "    H = \n",
        "    \n",
        "    exp_H = \n",
        "    pi = \n",
        "\n",
        "    action = \n",
        "\n",
        "    reward = \n",
        "    \n",
        "    if baseline:\n",
        "      b = \n",
        "\n",
        "    N[action] =\n",
        "    Q[action] = \n",
        "\n",
        "    # Track metrics\n",
        "    R_over_t.append(reward)\n",
        "    \n",
        "    total_R = \n",
        "    avg_R = \n",
        "    total_R_over_t.append(avg_R)\n",
        "\n",
        "    est_is_best = \n",
        "    est_is_best_over_t.append(est_is_best)\n",
        "\n",
        "    regret = \n",
        "    l_over_t.append(regret)\n",
        "\n",
        "    total_l = \n",
        "    total_l_over_t.append(total_l)\n",
        "\n",
        "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqUx-AYVvu2B"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MeaOB9RDAeLj",
        "outputId": "911616c5-5078-4a62-dad1-31bace65d4b3"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "\n",
        "temperatures = \n",
        "num_steps = \n",
        "\n",
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
        "\n",
        "for temperature in temperatures:\n",
        "\n",
        "  # arrays of the data generated from 100 runs\n",
        "  R_over_t_runs = []\n",
        "  total_R_over_t_runs = []\n",
        "  est_is_best_over_t_runs = []\n",
        "  l_over_t_runs = []\n",
        "  total_l_over_t_runs = []\n",
        "\n",
        "  for run in range(100):\n",
        "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = gradient_bandit(\n",
        "        three_arm_bernoulli_bandit, temperature, num_steps=num_steps, baseline=True)\n",
        "\n",
        "    R_over_t_runs.append(R_over_t)\n",
        "    total_R_over_t_runs.append(total_R_over_t)\n",
        "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
        "    l_over_t_runs.append(l_over_t)\n",
        "    total_l_over_t_runs.append(total_l_over_t)\n",
        "\n",
        "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
        "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
        "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
        "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
        "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
        "\n",
        "  # Plot 1: mean reward over time\n",
        "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
        "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(100)\n",
        "\n",
        "  axs[0,0].plot(mean_R_over_t_runs, label=rf\"$T = {temperature}$\")\n",
        "  axs[0,0].fill_between(range(num_steps), mean_R_over_t_runs - std_err_R_over_t_runs, \n",
        "                        mean_R_over_t_runs + std_err_R_over_t_runs, alpha=0.4)\n",
        "\n",
        "  axs[0,0].legend()\n",
        "  axs[0,0].set_xlabel(\"time step\")\n",
        "  axs[0,0].set_ylabel(\"reward\")\n",
        "  axs[0,0].set_title(\"Average Instantaneous Reward over Time\", y=-0.18)\n",
        "\n",
        "  # Plot 2: average reward up to time step t\n",
        "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
        "  std_err_total_R = np.std(total_R_over_t_runs, axis=0) / np.sqrt(100)\n",
        "\n",
        "  axs[0,1].plot(mean_total_R_over_t_runs, label=rf\"$T = {temperature}$\")\n",
        "  axs[0,1].fill_between(range(num_steps), mean_total_R_over_t_runs - std_err_total_R,\n",
        "                        mean_total_R_over_t_runs + std_err_total_R, alpha=0.4)\n",
        "\n",
        "  axs[0,1].legend()\n",
        "  axs[0,1].set_xlabel(\"time step\")\n",
        "  axs[0,1].set_ylabel(\"average reward\")\n",
        "  axs[0,1].set_title(\"Average Reward up to Time Step t\", y=-0.18)\n",
        "\n",
        "  # Plot 3: fraction optimal action\n",
        "  est_is_best_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
        "  axs[1,0].plot(est_is_best_avgs, label=rf\"$T = {temperature}$\")\n",
        "\n",
        "  axs[1,0].legend()\n",
        "  axs[1,0].set_xlabel(\"time step\")\n",
        "  axs[1,0].set_ylabel(\"fraction\")\n",
        "  axs[1,0].set_title(\"Fraction of Time Steps where Action Taken = Optimal\", y=-0.18)\n",
        "\n",
        "  # Plot 4: instantaneous regret\n",
        "  l_over_t_avgs = np.mean(l_over_t_runs, axis=0)\n",
        "  axs[1,1].plot(l_over_t_avgs, label=rf\"$T = {temperature}$\")\n",
        "\n",
        "  axs[1,1].legend()\n",
        "  axs[1,1].set_xlabel(\"time step\")\n",
        "  axs[1,1].set_ylabel(\"regret\")\n",
        "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
        "\n",
        "  # Plot 5: total regret\n",
        "  total_l_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
        "  axs[2,0].plot(total_l_avgs, label=rf\"$T = {temperature}$\")\n",
        "\n",
        "  axs[2,0].legend()\n",
        "  axs[2,0].set_xlabel(\"time step\")\n",
        "  axs[2,0].set_ylabel(\"regret\")\n",
        "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
        "\n",
        "axs[-1, -1].axis('off')\n",
        "\n",
        "fig.suptitle(\"Boltzmann Exploration with Varying Temperature\", fontsize=16, y=0.08)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NABR2XVSc2fk"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdtZT_3LNShd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9GklWhZM9um"
      },
      "source": [
        "## Q7 Thompson Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAlN8q-YM_Mt"
      },
      "outputs": [],
      "source": [
        "def Thompson_sampling(bandit, alpha_prior=1, beta_prior=1, T=1000):\n",
        "  \"\"\"Thompson Sampling for Bernoulli bandit using Beta priors.  \"\"\"\n",
        "  #TODO:\n",
        "\n",
        "  num_arms = \n",
        "  \n",
        "  # Initialize Beta distribution parameters for each arm\n",
        "  alpha = \n",
        "  beta = \n",
        "  \n",
        "  R_over_t = []\n",
        "  total_R_over_t = []\n",
        "  est_is_best_over_t = []\n",
        "  l_over_t = []\n",
        "  total_l_over_t = []\n",
        "  total_R = \n",
        "  total_l = \n",
        "  \n",
        "  for t in range(T):\n",
        "    optimal_arm = \n",
        "    optimal_reward = \n",
        "\n",
        "    theta = \n",
        "    \n",
        "    max_theta = \n",
        "    action = \n",
        "    \n",
        "    reward =\n",
        "    \n",
        "    # Update Beta posterior\n",
        "    alpha[action] += \n",
        "    beta[action] += \n",
        "    \n",
        "    # Track metrics\n",
        "    R_over_t.append(reward)\n",
        "    \n",
        "    total_R = \n",
        "    avg_R = \n",
        "    total_R_over_t.append(avg_R)\n",
        "    \n",
        "    est_is_best = \n",
        "    est_is_best_over_t.append(est_is_best)\n",
        "    \n",
        "    regret = \n",
        "    l_over_t.append(regret)\n",
        "    \n",
        "    total_l = \n",
        "    total_l_over_t.append(total_l)\n",
        "  \n",
        "  return R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3KAgC5v2gl"
      },
      "source": [
        "### Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E-2AQ2rx9xei",
        "outputId": "3e75cd61-e665-4af5-baf0-3c830c5099c2"
      },
      "outputs": [],
      "source": [
        "  #TODO:\n",
        "priors = []  \n",
        "prior_names = [r\"Beta(1,1) Uninformative\", r\"Beta(10,1) Optimistic\", r\"Beta(1,10) Pessimistic\"]\n",
        "T = \n",
        "\n",
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18, 18))\n",
        "\n",
        "for prior, prior_name in zip(priors, prior_names):\n",
        "  alpha_prior, beta_prior = prior\n",
        "  \n",
        "  # arrays of the data generated from 100 runs\n",
        "  R_over_t_runs = []\n",
        "  total_R_over_t_runs = []\n",
        "  est_is_best_over_t_runs = []\n",
        "  l_over_t_runs = []\n",
        "  total_l_over_t_runs = []\n",
        "\n",
        "  for run in range(100):\n",
        "    R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = Thompson_sampling(\n",
        "        three_arm_bernoulli_bandit, alpha_prior=alpha_prior, beta_prior=beta_prior, T=T)\n",
        "\n",
        "    R_over_t_runs.append(R_over_t)\n",
        "    total_R_over_t_runs.append(total_R_over_t)\n",
        "    est_is_best_over_t_runs.append(est_is_best_over_t)\n",
        "    l_over_t_runs.append(l_over_t)\n",
        "    total_l_over_t_runs.append(total_l_over_t)\n",
        "\n",
        "  R_over_t_runs = np.asarray(R_over_t_runs)\n",
        "  total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
        "  est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
        "  l_over_t_runs = np.asarray(l_over_t_runs)\n",
        "  total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
        "\n",
        "  # Plot 1: mean reward over time\n",
        "  mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
        "  std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(100)\n",
        "\n",
        "  axs[0,0].plot(mean_R_over_t_runs, label=prior_name)\n",
        "  axs[0,0].fill_between(range(T), mean_R_over_t_runs - std_err_R_over_t_runs, \n",
        "                        mean_R_over_t_runs + std_err_R_over_t_runs, alpha=0.4)\n",
        "\n",
        "  axs[0,0].legend()\n",
        "  axs[0,0].set_xlabel(\"time step\")\n",
        "  axs[0,0].set_ylabel(\"reward\")\n",
        "  axs[0,0].set_title(\"Average Instantaneous Reward over Time\", y=-0.18)\n",
        "\n",
        "  # Plot 2: average reward up to time step t\n",
        "  mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
        "  std_err_total_R = np.std(total_R_over_t_runs, axis=0) / np.sqrt(100)\n",
        "\n",
        "  axs[0,1].plot(mean_total_R_over_t_runs, label=prior_name)\n",
        "  axs[0,1].fill_between(range(T), mean_total_R_over_t_runs - std_err_total_R,\n",
        "                        mean_total_R_over_t_runs + std_err_total_R, alpha=0.4)\n",
        "\n",
        "  axs[0,1].legend()\n",
        "  axs[0,1].set_xlabel(\"time step\")\n",
        "  axs[0,1].set_ylabel(\"average reward\")\n",
        "  axs[0,1].set_title(\"Average Reward up to Time Step t\", y=-0.18)\n",
        "\n",
        "  # Plot 3: fraction optimal action\n",
        "  est_is_best_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
        "  axs[1,0].plot(est_is_best_avgs, label=prior_name)\n",
        "\n",
        "  axs[1,0].legend()\n",
        "  axs[1,0].set_xlabel(\"time step\")\n",
        "  axs[1,0].set_ylabel(\"fraction\")\n",
        "  axs[1,0].set_title(\"Fraction of Time Steps where Action Taken = Optimal\", y=-0.18)\n",
        "\n",
        "  # Plot 4: instantaneous regret\n",
        "  l_over_t_avgs = np.mean(l_over_t_runs, axis=0)\n",
        "  axs[1,1].plot(l_over_t_avgs, label=prior_name)\n",
        "\n",
        "  axs[1,1].legend()\n",
        "  axs[1,1].set_xlabel(\"time step\")\n",
        "  axs[1,1].set_ylabel(\"regret\")\n",
        "  axs[1,1].set_title(\"Instantaneous Regret over Time\", y=-0.18)\n",
        "\n",
        "  # Plot 5: total regret\n",
        "  total_l_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
        "  axs[2,0].plot(total_l_avgs, label=prior_name)\n",
        "\n",
        "  axs[2,0].legend()\n",
        "  axs[2,0].set_xlabel(\"time step\")\n",
        "  axs[2,0].set_ylabel(\"regret\")\n",
        "  axs[2,0].set_title(\"Total Regret up to Time Step t\", y=-0.18)\n",
        "\n",
        "axs[-1, -1].axis('off')\n",
        "\n",
        "fig.suptitle(\"Thompson Sampling with Different Beta Priors\", fontsize=16, y=0.08)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRc45fxdc46B"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROHiRPusMo37"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFBJd8hRHDbj"
      },
      "source": [
        "## Q9 Non-stationary Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbNLZxmiSLnP"
      },
      "outputs": [],
      "source": [
        "  #TODO:\n",
        "class Non_stationary_Bandit:\n",
        "    \"\"\"Non-stationary Bernoulli bandit where arm probabilities swap every 500 steps.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_arms=3, probs=None, delta=):\n",
        "        self.num_arms = \n",
        "        self.delta = \n",
        "        self.probs = \n",
        "        self.t = \n",
        "\n",
        "    def sample(self, arm_index):\n",
        "        # Sample reward using current probs\n",
        "        reward = \n",
        "        self.t += \n",
        "        \n",
        "        # Check if we need to swap for NEXT time step\n",
        "        # Swap at t=500, 1000, 1500 (after 500, 1000, 1500 samples)\n",
        "        \n",
        "        return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ5aVJBqc-8j"
      },
      "source": [
        "### Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for Q9\n",
        "#TODO:\n",
        "delta = \n",
        "T = \n",
        "num_runs = \n",
        "\n",
        "algorithms = [\n",
        "    (\"ε=1/8, α=0.1\", \"epsilon_greedy\", {}),\n",
        "    (\"ε=1/8, averaging\", \"epsilon_greedy\", {}),\n",
        "    (\"decay ε, α=0.1\", \"epsilon_greedy\", {}),\n",
        "    (\"Gradient T=0.1\", \"gradient_bandit\", {}),\n",
        "    (\"Gradient T=0.5\", \"gradient_bandit\", {}),\n",
        "    (\"Thompson Beta(1,1)\", \"thompson_sampling\", {}),\n",
        "]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(14, 25))\n",
        "\n",
        "for alg_name, alg_type, alg_params in algorithms:\n",
        "\n",
        "    # arrays of the data generated from 100 runs\n",
        "    R_over_t_runs = []\n",
        "    total_R_over_t_runs = []\n",
        "    est_is_best_over_t_runs = []\n",
        "    l_over_t_runs = []\n",
        "    total_l_over_t_runs = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        # Create a fresh non-stationary bandit for each run\n",
        "        bandit = Non_stationary_Bandit(num_arms=3, probs=[0.5, 0.5 - delta, 0.5 + delta], delta=delta)\n",
        "        \n",
        "        if alg_type == \"epsilon_greedy\":\n",
        "            R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = epsilon_greedy(\n",
        "                bandit, \n",
        "                epsilon=alg_params[\"epsilon\"],\n",
        "                alpha=alg_params.get(\"alpha\"),\n",
        "                num_time_step=T,\n",
        "                epsilon_decay=alg_params.get(\"epsilon_decay\", False),\n",
        "                lambda_=alg_params.get(\"lambda_\", 0.001)\n",
        "            )\n",
        "        elif alg_type == \"gradient_bandit\":\n",
        "            R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = gradient_bandit(\n",
        "                bandit,\n",
        "                temperature=alg_params[\"temperature\"],\n",
        "                num_steps=T,\n",
        "                baseline=True\n",
        "            )\n",
        "        elif alg_type == \"thompson_sampling\":\n",
        "            R_over_t, total_R_over_t, est_is_best_over_t, l_over_t, total_l_over_t = Thompson_sampling(\n",
        "                bandit,\n",
        "                alpha_prior=1,\n",
        "                beta_prior=1,\n",
        "                T=T\n",
        "            )\n",
        "\n",
        "        R_over_t_runs.append(R_over_t)\n",
        "        total_R_over_t_runs.append(total_R_over_t)\n",
        "        est_is_best_over_t_runs.append(est_is_best_over_t)\n",
        "        l_over_t_runs.append(l_over_t)\n",
        "        total_l_over_t_runs.append(total_l_over_t)\n",
        "\n",
        "    R_over_t_runs = np.asarray(R_over_t_runs)\n",
        "    total_R_over_t_runs = np.asarray(total_R_over_t_runs)\n",
        "    est_is_best_over_t_runs = np.asarray(est_is_best_over_t_runs)\n",
        "    l_over_t_runs = np.asarray(l_over_t_runs)\n",
        "    total_l_over_t_runs = np.asarray(total_l_over_t_runs)\n",
        "\n",
        "    # plot the mean reward over time\n",
        "    mean_R_over_t_runs = np.mean(R_over_t_runs, axis=0)\n",
        "    std_err_R_over_t_runs = np.std(R_over_t_runs, axis=0) / np.sqrt(num_runs)\n",
        "\n",
        "    axs[0].plot(mean_R_over_t_runs, label=alg_name)\n",
        "    axs[0].fill_between(range(T), mean_R_over_t_runs - std_err_R_over_t_runs, \n",
        "                        mean_R_over_t_runs + std_err_R_over_t_runs, alpha=0.4)\n",
        "\n",
        "    axs[0].legend()\n",
        "    axs[0].set_xlabel(\"time step\")\n",
        "    axs[0].set_ylabel(\"reward value\")\n",
        "    axs[0].set_title(\"Average Instantaneous Reward Received over Time\")\n",
        "\n",
        "    # plot the mean average reward up to time step t\n",
        "    mean_total_R_over_t_runs = np.mean(total_R_over_t_runs, axis=0)\n",
        "    std_err_total_R_over_t_runs = np.std(total_R_over_t_runs, axis=0) / np.sqrt(num_runs)\n",
        "\n",
        "    axs[1].plot(mean_total_R_over_t_runs, label=alg_name)\n",
        "    axs[1].fill_between(range(T), mean_total_R_over_t_runs - std_err_total_R_over_t_runs, \n",
        "                        mean_total_R_over_t_runs + std_err_total_R_over_t_runs, alpha=0.4)\n",
        "\n",
        "    axs[1].legend()\n",
        "    axs[1].set_xlabel(\"time step\")\n",
        "    axs[1].set_ylabel(\"average reward\")\n",
        "    axs[1].set_title(\"Average Reward up to Time Step t\")\n",
        "\n",
        "    # plot the fraction of optimal action\n",
        "    est_is_best_over_t_runs_avgs = np.mean(est_is_best_over_t_runs, axis=0)\n",
        "    axs[2].plot(est_is_best_over_t_runs_avgs, label=alg_name)\n",
        "\n",
        "    axs[2].legend()\n",
        "    axs[2].set_xlabel(\"time step\")\n",
        "    axs[2].set_ylabel(\"fraction\")\n",
        "    axs[2].set_title(\"Fraction of Runs where Action Taken is Optimal\")\n",
        "\n",
        "    # plot the mean instantaneous regret over time\n",
        "    l_over_t_runs_avgs = np.mean(l_over_t_runs, axis=0)\n",
        "    axs[3].plot(l_over_t_runs_avgs, label=alg_name)\n",
        "\n",
        "    axs[3].legend()\n",
        "    axs[3].set_xlabel(\"time step\")\n",
        "    axs[3].set_ylabel(\"regret\")\n",
        "    axs[3].set_title(\"Instantaneous Regret over Time\")\n",
        "\n",
        "    # plot the total regret over time\n",
        "    total_l_over_t_runs_avgs = np.mean(total_l_over_t_runs, axis=0)\n",
        "    axs[4].plot(total_l_over_t_runs_avgs, label=alg_name)\n",
        "\n",
        "    axs[4].legend()\n",
        "    axs[4].set_xlabel(\"time step\")\n",
        "    axs[4].set_ylabel(\"regret\")\n",
        "    axs[4].set_title(\"Total Regret up to Time Step t\")\n",
        "\n",
        "# Add vertical lines at probability swap points\n",
        "for ax in axs:\n",
        "    for swap_t in [500, 1000, 1500]:\n",
        "        ax.axvline(x=swap_t, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "fig.suptitle('Non-stationary Bandit: Probabilities Swap Every 500 Steps', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VG3KAgC5v2gl",
        "nRc45fxdc46B"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
